{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shravanimamidala/LunarLanding_DLProject/blob/main/LunarLander_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ECGFbabrIdLS",
        "outputId": "7e82dbe9-0504-4a02-deb9-18a024457376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Box2D\n",
            "  Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n",
            "Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Box2D\n",
            "Successfully installed Box2D-2.3.10\n",
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.1.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable_baselines3) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable_baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable_baselines3-2.6.0\n",
            "Collecting shimmy>=2.0\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from shimmy>=2.0) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.11/dist-packages (from shimmy>=2.0) (1.1.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (0.0.4)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: shimmy\n",
            "Successfully installed shimmy-2.0.0\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: moviePy in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviePy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.11/dist-packages (from moviePy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from moviePy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviePy) (0.1.11)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from moviePy) (2.0.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviePy) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviePy) (0.6.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio<3.0,>=2.5->moviePy) (11.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviePy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviePy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviePy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviePy) (2025.1.31)\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "#Cell 0\n",
        "\n",
        "!pip install Box2D\n",
        "!pip install stable_baselines3\n",
        "!pip install 'shimmy>=2.0'\n",
        "!pip install pygame\n",
        "!pip install moviePy\n",
        "!pip install --upgrade stable-baselines3 gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Imports\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import math\n",
        "import uuid\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import moviepy\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "    print(f\"Gymnasium version: {gym.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"⚠ Error: 'gymnasium' package is not installed. Install it using: !pip install gymnasium\")\n",
        "    raise ImportError(\"Gymnasium is required for this notebook.\")\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
        "# Verify Stable Baselines3\n",
        "print(f\"Stable Baselines3 version: {PPO.__module__.split('.')[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJIL5fVsInGc",
        "outputId": "c207433b-d3ee-48f6-e324-69a9b13f620d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gymnasium version: 1.1.1\n",
            "Stable Baselines3 version: stable_baselines3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Configuration & Hyperparameters\n",
        "ENV_NAME = \"LunarLander-v3\"\n",
        "# Reproducibility\n",
        "SEEDS = [0, 1, 2]\n",
        "def set_global_seed(seed: int):\n",
        "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "TRAIN_MAX_STEPS = 1000\n",
        "EVAL_MAX_STEPS = 250\n",
        "\n",
        "# Training schedule\n",
        "EPISODES = 250\n",
        "MILESTONES = [10,50,100,150,250]\n",
        "\n",
        "# # Smoke Test schedule\n",
        "# EPISODES = 5\n",
        "# MILESTONES = [1,5]\n",
        "\n",
        "# Replay buffer and optimization\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 50_000\n",
        "\n",
        "# Epsilon-greedy schedule\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.01\n",
        "EPS_DECAY = 5e-4\n",
        "\n",
        "# Discount factor and target update\n",
        "GAMMA = 0.99\n",
        "TARGET_UPDATE_FREQ = 1_000\n",
        "\n",
        "# Algorithm-specific configurations\n",
        "CONFIG = {\n",
        "    'vanilla_dqn': {'lr': 1e-3, 'batch_size': 64, 'prioritized': False},\n",
        "    'double_dqn': {'lr': 1e-3, 'batch_size': 64, 'prioritized': False},\n",
        "    'dueling_dqn': {'lr': 1e-3, 'batch_size': 64, 'prioritized': False},\n",
        "    'per_dqn': {'lr': 1e-3, 'batch_size': 64, 'prioritized': True, 'alpha': 0.6, 'beta': 0.4},\n",
        "    'ppo': {'n_steps': 1000, 'learning_rate': 3e-4, 'batch_size' : 50}\n",
        "}\n",
        "# Directories\n",
        "BASE_LOG_DIR = \"logs\"\n",
        "BASE_VIDEO_DIR = \"videos\"\n",
        "PLOTS_DIR = \"plots\"\n",
        "os.makedirs(BASE_LOG_DIR, exist_ok=True)\n",
        "os.makedirs(BASE_VIDEO_DIR, exist_ok=True)\n",
        "os.makedirs(PLOTS_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "xLUDbLBbIvWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree(BASE_LOG_DIR, ignore_errors=True)\n",
        "os.makedirs(BASE_LOG_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "sIkXmmwHVMHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Replay Buffer\n",
        "from collections import namedtuple\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int, prioritized: bool = False, alpha: float = 0.6, beta: float = 0.4):\n",
        "        self.capacity     = capacity\n",
        "        self.prioritized  = prioritized\n",
        "        self.alpha        = alpha\n",
        "        self.beta         = beta\n",
        "        self.buffer       = []\n",
        "        self.priorities   = []\n",
        "        self.position     = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition.\"\"\"\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(Transition(*args))\n",
        "            self.priorities.append(max(self.priorities, default=1.0))\n",
        "        else:\n",
        "            self.buffer[self.position]     = Transition(*args)\n",
        "            self.priorities[self.position] = max(self.priorities)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size: int):\n",
        "        \"\"\"\n",
        "        Sample a batch of transitions, with optional prioritized replay.\n",
        "        Returns: (transitions, indices, weights)\n",
        "        \"\"\"\n",
        "        if self.prioritized and len(self.buffer) >= batch_size:\n",
        "            # compute sampling probabilities\n",
        "            probs = np.array(self.priorities, dtype=np.float32) ** self.alpha\n",
        "            probs /= probs.sum()\n",
        "            indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
        "            weights = (len(self.buffer) * probs[indices]) ** (-self.beta)\n",
        "            weights /= weights.max()\n",
        "            samples = [self.buffer[i] for i in indices]\n",
        "            return samples, indices, torch.tensor(weights, dtype=torch.float32)\n",
        "        else:\n",
        "            samples = random.sample(self.buffer, batch_size)\n",
        "            return samples, None, None\n",
        "\n",
        "    def update_priorities(self, indices, errors):\n",
        "        \"\"\"Update priorities of sampled transitions.\"\"\"\n",
        "        for idx, err in zip(indices, errors):\n",
        "            self.priorities[idx] = abs(err) + 1e-6\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "DYK-a4kgIzFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Q-Networks\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "class DuelingQNetwork(nn.Module):\n",
        "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
        "        super(DuelingQNetwork, self).__init__()\n",
        "        # value stream\n",
        "        self.value_net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        # advantage stream\n",
        "        self.adv_net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        value = self.value_net(x)\n",
        "        adv   = self.adv_net(x)\n",
        "        # broadcast value and normalize advantage\n",
        "        return value + (adv - adv.mean(dim=1, keepdim=True))"
      ],
      "metadata": {
        "id": "2f6ThheXJIdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: DQN Agent Base\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim: int, action_dim: int, variant: str, config: dict):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.variant = variant\n",
        "        self.config = config\n",
        "        # Verify dimensions\n",
        "        assert state_dim == 8, f\"Expected state_dim=8, got {state_dim}\"\n",
        "        assert action_dim == 4, f\"Expected action_dim=4, got {action_dim}\"\n",
        "        # Choose network architecture\n",
        "        if variant == \"dueling\":\n",
        "            self.q_net = DuelingQNetwork(state_dim, action_dim)\n",
        "            self.target_net = DuelingQNetwork(state_dim, action_dim)\n",
        "        else:\n",
        "            self.q_net = QNetwork(state_dim, action_dim)\n",
        "            self.target_net = QNetwork(state_dim, action_dim)\n",
        "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "        # Replay buffer\n",
        "        prioritized = config.get('prioritized', False)\n",
        "        alpha = config.get('alpha', 0.6)\n",
        "        beta = config.get('beta', 0.4)\n",
        "        self.buffer = ReplayBuffer(BUFFER_SIZE, prioritized=prioritized, alpha=alpha, beta=beta)\n",
        "        # Optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=config['lr'])\n",
        "        # Counters\n",
        "        self.steps_done = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # Epsilon-greedy\n",
        "        eps = EPS_END + (EPS_START - EPS_END) * math.exp(-self.steps_done * EPS_DECAY)\n",
        "        self.steps_done += 1\n",
        "        if random.random() < eps:\n",
        "            return random.randrange(self.action_dim)\n",
        "        state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            return int(self.q_net(state_t).argmax(dim=1).item())\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "    def train_step(self):\n",
        "        # ————————————\n",
        "        # Beta annealing for PER‑DQN:\n",
        "        # linearly increase buffer.beta from 0.4 → 1.0 over 250 000 steps\n",
        "        if self.variant == \"per\":\n",
        "            self.buffer.beta = min(1.0, 0.4 + (1.0 - 0.4) * (self.steps_done / 250_000))\n",
        "        # ————————————\n",
        "        if len(self.buffer) < self.config['batch_size']:\n",
        "            return None\n",
        "        transitions, indices, weights = self.buffer.sample(self.config['batch_size'])\n",
        "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
        "        s_v = torch.tensor(np.stack(states), dtype=torch.float32)\n",
        "        a_v = torch.tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
        "        r_v = torch.tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n",
        "        ns_v = torch.tensor(np.stack(next_states), dtype=torch.float32)\n",
        "        d_v = torch.tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
        "        q_current = self.q_net(s_v).gather(1, a_v)\n",
        "        with torch.no_grad():\n",
        "            if self.variant == \"double\":\n",
        "                next_actions = self.q_net(ns_v).argmax(dim=1, keepdim=True)\n",
        "                q_next = self.target_net(ns_v).gather(1, next_actions)\n",
        "            else:\n",
        "                q_next = self.target_net(ns_v).max(1, keepdim=True)[0]\n",
        "            q_target = r_v + GAMMA * (1 - d_v) * q_next\n",
        "        if self.buffer.prioritized and indices is not None:\n",
        "            errors = (q_current - q_target).squeeze().abs().detach().cpu().numpy()\n",
        "            self.buffer.update_priorities(indices, errors)\n",
        "            loss = ((q_current - q_target).pow(2) * weights.unsqueeze(1)).mean()\n",
        "        else:\n",
        "            loss = F.mse_loss(q_current, q_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        # Periodic target update\n",
        "        if self.steps_done % TARGET_UPDATE_FREQ == 0:\n",
        "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "        return loss.item()"
      ],
      "metadata": {
        "id": "SqCvfAU-JLjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Vanilla DQN Factory\n",
        "def make_vanilla_dqn_agent(env=None):\n",
        "    \"\"\"Instantiate a Vanilla DQN agent.\"\"\"\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    assert state_dim == 8, f\"Expected state_dim=8, got {state_dim}\"\n",
        "    assert action_dim == 4, f\"Expected action_dim=4, got {action_dim}\"\n",
        "    return DQNAgent(state_dim, action_dim, variant=\"vanilla\", config=CONFIG['vanilla_dqn'])"
      ],
      "metadata": {
        "id": "el4ao_IgJsph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Double DQN Factory\n",
        "def make_double_dqn_agent(env=None):\n",
        "    \"\"\"Instantiate a Double DQN agent.\"\"\"\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    assert state_dim == 8, f\"Expected state_dim=8, got {state_dim}\"\n",
        "    assert action_dim == 4, f\"Expected action_dim=4, got {action_dim}\"\n",
        "    return DQNAgent(state_dim, action_dim, variant=\"double\", config=CONFIG['double_dqn'])"
      ],
      "metadata": {
        "id": "YsiHPctUJvte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Dueling DQN Factory\n",
        "def make_dueling_dqn_agent(env=None):\n",
        "    \"\"\"Instantiate a Dueling DQN agent.\"\"\"\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    assert state_dim == 8, f\"Expected state_dim=8, got {state_dim}\"\n",
        "    assert action_dim == 4, f\"Expected action_dim=4, got {action_dim}\"\n",
        "    return DQNAgent(state_dim, action_dim, variant=\"dueling\", config=CONFIG['dueling_dqn'])"
      ],
      "metadata": {
        "id": "BFdClsx3J1GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: PER DQN Factory\n",
        "def make_per_dqn_agent(env=None):\n",
        "    \"\"\"Instantiate a Prioritized Experience Replay DQN agent.\"\"\"\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    assert state_dim == 8, f\"Expected state_dim=8, got {state_dim}\"\n",
        "    assert action_dim == 4, f\"Expected action_dim=4, got {action_dim}\"\n",
        "    return DQNAgent(state_dim, action_dim, variant=\"per\", config=CONFIG['per_dqn'])"
      ],
      "metadata": {
        "id": "IPLF53uTJ34K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Cell 10: PPO Agent Factory\n",
        "\n",
        "# from stable_baselines3 import PPO\n",
        "# from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# class PPOAgentWrapper:\n",
        "#     def __init__(self, env):\n",
        "#         \"\"\"env: either a callable factory or a Gym env instance\"\"\"\n",
        "#         # Verify dimensions\n",
        "#         state_dim = env.observation_space.shape[0]\n",
        "#         action_dim = env.action_space.n\n",
        "#         assert state_dim == 8, f\"Expected state_dim=8, got {state_dim}\"\n",
        "#         assert action_dim == 4, f\"Expected action_dim=4, got {action_dim}\"\n",
        "#         # Wrap env for DummyVecEnv\n",
        "#         if callable(env):\n",
        "#             env_fn = env\n",
        "#         else:\n",
        "#             env_fn = lambda: env\n",
        "#         self.vec_env = DummyVecEnv([env_fn])\n",
        "#         self.model = PPO(\n",
        "#             \"MlpPolicy\",\n",
        "#             self.vec_env,\n",
        "#             verbose=0,\n",
        "#             n_steps=CONFIG['ppo']['n_steps'],\n",
        "#             learning_rate=CONFIG['ppo']['learning_rate']\n",
        "#         )\n",
        "\n",
        "\n",
        "#     def learn(self, total_timesteps, reset_num_timesteps=False):\n",
        "#         \"\"\"Train the model for a specified number of timesteps.\"\"\"\n",
        "#         self.model.learn(total_timesteps=total_timesteps, reset_num_timesteps=reset_num_timesteps)\n",
        "\n",
        "#     def predict(self, state, deterministic=True):\n",
        "#         # SB3 VecEnv expects batched inputs\n",
        "#         state_vec = np.array(state, dtype=np.float32)[None]\n",
        "#         action, _ = self.model.predict(state_vec, deterministic=deterministic)\n",
        "#         return int(action[0]), _\n",
        "\n",
        "# def make_ppo_agent(env=None):\n",
        "#     \"\"\"Instantiate a PPO agent wrapper.\"\"\"\n",
        "#     return PPOAgentWrapper(env)\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "def make_ppo_agent(env=None, seed=None):\n",
        "    \"\"\"Instantiate a PPO agent wrapper with seeded vectorized environment.\"\"\"\n",
        "    def _make_env():\n",
        "        env = make_env(seed=seed)  # Assuming make_env is defined elsewhere\n",
        "        return env\n",
        "    vec_env = DummyVecEnv([_make_env])\n",
        "    model = PPO(\n",
        "        \"MlpPolicy\",\n",
        "        vec_env,\n",
        "        verbose=0,\n",
        "        n_steps=CONFIG['ppo']['n_steps'],  # Assuming CONFIG is defined\n",
        "        batch_size=CONFIG['ppo'].get('batch_size', 64),\n",
        "        learning_rate=CONFIG['ppo']['learning_rate'],\n",
        "        seed=seed  # Ensure PPO model uses the same seed\n",
        "    )\n",
        "    return PPOAgentWrapper(model, vec_env)\n",
        "\n",
        "class PPOAgentWrapper:\n",
        "    def __init__(self, model, vec_env):\n",
        "        self.model = model\n",
        "        self.vec_env = vec_env\n",
        "\n",
        "    def learn(self, total_timesteps: int, reset_num_timesteps: bool = False, callback = None, **kwargs):\n",
        "        \"\"\"Train the model, forwarding any callback(s) to SB3.\"\"\"\n",
        "        return self.model.learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            reset_num_timesteps=reset_num_timesteps,\n",
        "            callback=callback,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def predict(self, state, deterministic=True):\n",
        "        state_vec = np.array(state, dtype=np.float32)[None]\n",
        "        action, _ = self.model.predict(state_vec, deterministic=deterministic)\n",
        "        return int(action[0]), _"
      ],
      "metadata": {
        "id": "STMkGfgaJ8I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Environment Factory\n",
        "class FuelTrackingWrapper(gym.Wrapper):\n",
        "    \"\"\"Wrapper to track fuel consumption based on actions.\"\"\"\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.initial_fuel = 1000.0  # Initial fuel for remaining fuel metric\n",
        "        self.fuel = 0.0\n",
        "        # Fuel costs: main engine (-0.3), side engines (-0.03)\n",
        "        self.fuel_costs = {0: 0.0, 1: -0.03, 2: -0.3, 3: -0.03}  # Actions: none, left, main, right\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        self.fuel += self.fuel_costs[action]\n",
        "        info['fuel'] = self.initial_fuel + self.fuel  # Remaining fuel\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.fuel = 0.0\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        info['fuel'] = self.initial_fuel\n",
        "        return obs, info\n",
        "\n",
        "def make_env(seed=None, record=False, record_path=None, video_prefix=None):\n",
        "    env = gymnasium.make(ENV_NAME, render_mode=\"rgb_array\")\n",
        "    # Verify state and action spaces\n",
        "    assert env.observation_space.shape[0] == 8, f\"Expected state_dim=8, got {env.observation_space.shape[0]}\"\n",
        "    assert env.action_space.n == 4, f\"Expected action_dim=4, got {env.action_space.n}\"\n",
        "    # Wrap with fuel tracking\n",
        "    env = FuelTrackingWrapper(env)\n",
        "    if seed is not None:\n",
        "        env.reset(seed=seed)\n",
        "    if record and record_path:\n",
        "        # ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
        "        prefix = video_prefix or f\"{ENV_NAME}\"\n",
        "        env = RecordVideo(env, video_folder=record_path, name_prefix=prefix, episode_trigger=lambda x: x in MILESTONES)\n",
        "    return env"
      ],
      "metadata": {
        "id": "kK0GVZ8kJ-Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Evaluation Utility (Updated)\n",
        "\n",
        "def evaluate(agent, is_ppo, episodes=10, record=False, record_path=None, video_prefix=None,return_raw=False, eval_seed: int = None):\n",
        "    stats = {\"reward\": [], \"success\": [], \"dist\": [], \"fuel\": []}\n",
        "\n",
        "\n",
        "    # for ep in range(1, episodes + 1):\n",
        "    #     # ─────────── prepare the env for this episode ───────────\n",
        "    #     if is_ppo:\n",
        "    #         # PPO uses a VecEnv; wrap it if we want video\n",
        "    #         vec_env = agent.vec_env\n",
        "    #         if record and record_path and video_prefix:\n",
        "    #             vec_env = VecVideoRecorder(\n",
        "    #                 vec_env,\n",
        "    #                 video_folder=record_path,\n",
        "    #                 record_video_trigger=lambda _: True,\n",
        "    #                 video_length=EVAL_MAX_STEPS,\n",
        "    #                 name_prefix=video_prefix\n",
        "    #             )\n",
        "    #         if eval_seed is not None:\n",
        "    #             vec_env.seed(eval_seed)\n",
        "    #         obs = vec_env.reset()\n",
        "    #         state = obs[0]\n",
        "    #     else:\n",
        "    #         # DQN uses a single-env; wrap with RecordVideo if requested\n",
        "    #         env = make_env(\n",
        "    #             seed=eval_seed,\n",
        "    #             record=record,\n",
        "    #             record_path=record_path,\n",
        "    #             video_prefix=video_prefix\n",
        "    #         )\n",
        "    #         if record and record_path and video_prefix:\n",
        "    #             env = RecordVideo(\n",
        "    #                 env,\n",
        "    #                 video_folder=record_path,\n",
        "    #                 name_prefix=video_prefix\n",
        "    #             )\n",
        "    #         state, _ = env.reset(seed=eval_seed)\n",
        "\n",
        "    # ─── Build & wrap the env ONE TIME ───\n",
        "    if is_ppo:\n",
        "        # start from the agent’s VecEnv\n",
        "        base_env = agent.vec_env\n",
        "        if record and record_path and video_prefix:\n",
        "            env = VecVideoRecorder(\n",
        "                base_env,\n",
        "                video_folder=record_path,\n",
        "                record_video_trigger=lambda step: step == 0,\n",
        "                video_length=EVAL_MAX_STEPS,\n",
        "                name_prefix=video_prefix,\n",
        "            )\n",
        "        else:\n",
        "            env = base_env\n",
        "    else:\n",
        "        # single‐env for DQN\n",
        "        base_env = make_env(\n",
        "            seed=eval_seed,\n",
        "            record=record,\n",
        "            record_path=record_path,\n",
        "            video_prefix=video_prefix,\n",
        "        )\n",
        "        if record and record_path and video_prefix:\n",
        "            env = RecordVideo(\n",
        "                base_env,\n",
        "                video_folder=record_path,\n",
        "                name_prefix=video_prefix,\n",
        "            )\n",
        "        else:\n",
        "            env = base_env\n",
        "\n",
        "    # ─── Run N episodes ───\n",
        "    for ep in range(episodes):\n",
        "        # reseed if requested\n",
        "        if eval_seed is not None and is_ppo:\n",
        "            # VecEnv supports .seed()\n",
        "            env.seed(eval_seed)\n",
        "\n",
        "        # reset\n",
        "        if is_ppo:\n",
        "            obs = env.reset()       # no seed arg on VecEnv\n",
        "            state = obs[0]\n",
        "        else:\n",
        "            state, _ = env.reset(seed=eval_seed)\n",
        "\n",
        "        total_r, total_f = 0.0, 0.0\n",
        "        done = False\n",
        "\n",
        "        for t in range(EVAL_MAX_STEPS):\n",
        "            if is_ppo:\n",
        "                action, _ = agent.predict(state)\n",
        "                obs, rewards, dones, infos = env.step([action])\n",
        "                ns, r, done, info = obs[0], rewards[0], dones[0], infos[0]\n",
        "            else:\n",
        "                action = agent.select_action(state)\n",
        "                ns, r, term, trunc, info = env.step(action)\n",
        "                done = term or trunc\n",
        "\n",
        "            total_r += r\n",
        "            total_f   = info.get(\"fuel\", 0)\n",
        "            state     = ns\n",
        "\n",
        "            if done:\n",
        "                stats[\"success\"].append(1 if total_r >= 200 else 0)\n",
        "                stats[\"dist\"].append(math.hypot(state[0], state[1]))\n",
        "                break\n",
        "\n",
        "        if not done:\n",
        "            stats[\"success\"].append(0)\n",
        "            stats[\"dist\"].append(math.hypot(state[0], state[1]))\n",
        "\n",
        "        stats[\"reward\"].append(total_r)\n",
        "        stats[\"fuel\"].append(total_f)\n",
        "\n",
        "    # ─── Clean up ───\n",
        "    if record:\n",
        "        env.close()\n",
        "    if not is_ppo:\n",
        "        base_env.close()\n",
        "\n",
        "    if return_raw:\n",
        "        return stats[\"reward\"]\n",
        "\n",
        "    mean_r, std_r = np.mean(stats[\"reward\"]), np.std(stats[\"reward\"])\n",
        "    mean_d, std_d = np.mean(stats[\"dist\"]),   np.std(stats[\"dist\"])\n",
        "    return {\n",
        "        \"mean_reward\":  f\"{mean_r:.2f} ± {std_r:.2f}\",\n",
        "        \"success_rate\": np.mean(stats[\"success\"]),\n",
        "        \"mean_dist\":    f\"{mean_d:.2f} ± {std_d:.2f}\",\n",
        "        \"mean_fuel\":    np.mean(stats[\"fuel\"]),\n",
        "        \"std_reward\":   std_r,\n",
        "        \"std_dist\":     std_d,\n",
        "    }"
      ],
      "metadata": {
        "id": "yHQzKCggKWjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Training & Milestone Logging\n",
        "\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "class StepCounterCallback(BaseCallback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.total_steps = 0\n",
        "    def _on_step(self) -> bool:\n",
        "        self.total_steps += 1\n",
        "        return True\n",
        "\n",
        "class TrainRewardCallback(BaseCallback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.episode_rewards = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Required stub so callback is concrete\n",
        "        return True\n",
        "\n",
        "    def _on_rollout_end(self) -> bool:\n",
        "        # Sum up the rewards from this rollout segment\n",
        "        rewards = self.locals.get(\"rewards\", [])\n",
        "        self.episode_rewards.append(float(sum(rewards)))\n",
        "        return True\n",
        "\n",
        "def train_and_snapshot(name, agent_ctor, is_ppo):\n",
        "    \"\"\"Train an agent, log per-episode metrics, checkpoints, and milestone evaluations.\"\"\"\n",
        "    # Master log for full training\n",
        "    master_full_log = os.path.join(BASE_LOG_DIR, \"master_full_training.csv\")\n",
        "    master_milestone_log = os.path.join(BASE_LOG_DIR, \"master_milestone_metrics.csv\")\n",
        "\n",
        "    if not os.path.exists(master_full_log):\n",
        "      # create empty CSV with only column headers\n",
        "      pd.DataFrame(\n",
        "          columns=[\n",
        "              \"algorithm\",\"seed\",\"episode\",\"reward\",\n",
        "              \"steps\",\"time_sec\",\"fuel\",\n",
        "              \"sample_efficiency\",\"training_std\"\n",
        "            ]).to_csv(master_full_log, index=False)\n",
        "\n",
        "    if not os.path.exists(master_milestone_log):\n",
        "      # create empty CSV with only column headers\n",
        "      pd.DataFrame(\n",
        "          columns=[\n",
        "              \"algorithm\",\"seed\",\"episode\",\"mean_reward\",\n",
        "              \"success_rate\",\"mean_dist\",\"mean_fuel\",\n",
        "              \"sample_efficiency\",\"training_std\", \"eval_variance\",\n",
        "              \"time_sec\",\"gpu_bytes\"\n",
        "          ]).to_csv(master_milestone_log, index=False)\n",
        "\n",
        "    # Directory for milestone videos\n",
        "    tl_base = os.path.join(BASE_VIDEO_DIR, \"timeline_comparisons\")\n",
        "    os.makedirs(tl_base, exist_ok=True)\n",
        "    for m in MILESTONES:\n",
        "        os.makedirs(os.path.join(tl_base, f\"milestone_{m}_eps\"), exist_ok=True)\n",
        "\n",
        "    total_steps = 0\n",
        "    gpu_timer = GPUTimer()\n",
        "    for seed in SEEDS:\n",
        "        set_global_seed(seed)\n",
        "        if is_ppo:\n",
        "            agent = agent_ctor(seed=seed)\n",
        "        else:\n",
        "            env = make_env(seed)\n",
        "            agent = agent_ctor(env)\n",
        "\n",
        "        log_dir = os.path.join(BASE_LOG_DIR, name)\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "        episode_rewards = []\n",
        "        first_success_steps = float('inf')\n",
        "\n",
        "        for epi in range(1, EPISODES + 1):\n",
        "            start = time.time()\n",
        "            if is_ppo:\n",
        "                step_cb   = StepCounterCallback()\n",
        "                reward_cb = TrainRewardCallback()\n",
        "                agent.learn(total_timesteps=TRAIN_MAX_STEPS,reset_num_timesteps=False,callback=[step_cb, reward_cb])\n",
        "                # get reward from callback instead of separate eval\n",
        "                ep_reward = reward_cb.episode_rewards[-1]\n",
        "                # get fuel via a quick evaluation run\n",
        "                metrics   = evaluate(agent, True, episodes=1)\n",
        "                ep_fuel   = metrics[\"mean_fuel\"]\n",
        "                steps     = step_cb.total_steps\n",
        "                total_steps += step_cb.total_steps\n",
        "\n",
        "            else:\n",
        "                state, _ = env.reset()\n",
        "                ep_reward = 0\n",
        "                for t in range(TRAIN_MAX_STEPS):\n",
        "                    action = agent.select_action(state)\n",
        "                    ns, r, term, trunc, info = env.step(action)\n",
        "                    agent.store_transition(state, action, r, ns, term or trunc)\n",
        "                    agent.train_step()\n",
        "                    state = ns\n",
        "                    ep_reward += r\n",
        "                    total_steps += 1\n",
        "                    if term or trunc:\n",
        "                        break\n",
        "                steps = t + 1\n",
        "                ep_fuel = info['fuel']\n",
        "            elapsed = time.time() - start\n",
        "            episode_rewards.append(ep_reward)\n",
        "\n",
        "            # Sample efficiency: steps to reach reward >= 200\n",
        "            # sample_eff = total_steps if ep_reward >= 200 and total_steps > 0 else float('inf')\n",
        "            if ep_reward >= 200 and first_success_steps == float('inf'):\n",
        "                first_success_steps = total_steps\n",
        "            sample_eff = first_success_steps if first_success_steps < float('inf') else float('inf')\n",
        "\n",
        "            # Training stability: standard deviation of last 50 episodes\n",
        "            # reward_var = np.var(episode_rewards[-10:]) if len(episode_rewards) >= 10 else 0.0\n",
        "            training_std = float(np.std(episode_rewards[-50:])) if len(episode_rewards) >= 50 else float(np.std(episode_rewards))\n",
        "\n",
        "            # Append to master log\n",
        "            pd.DataFrame([{\n",
        "                \"algorithm\": name,\n",
        "                \"seed\": seed,\n",
        "                \"episode\": epi,\n",
        "                \"reward\": ep_reward,\n",
        "                \"steps\": steps,\n",
        "                \"time_sec\": elapsed,\n",
        "                \"fuel\": ep_fuel,\n",
        "                \"sample_efficiency\": sample_eff,\n",
        "                \"training_std\": training_std\n",
        "            }]).to_csv(master_full_log, mode=\"a\", header=False, index=False)\n",
        "\n",
        "            # Checkpoint DQN\n",
        "            if not is_ppo and epi == EPISODES:\n",
        "                ckpt = os.path.join(log_dir, f\"{name}_seed{seed}_final.pth\")\n",
        "                torch.save(agent.q_net.state_dict(), ckpt)\n",
        "\n",
        "            # ── SAVE THE TRAINED MODEL ──\n",
        "            if epi == EPISODES:\n",
        "                if is_ppo:\n",
        "                    # save the SB3 PPO model\n",
        "                    agent.model.save(os.path.join(log_dir, f\"{name}_seed{seed}_model.zip\"))\n",
        "                else:\n",
        "                    # save the DQN weights\n",
        "                    torch.save(\n",
        "                        agent.q_net.state_dict(),\n",
        "                        os.path.join(log_dir, f\"{name}_seed{seed}_qnet.pth\")\n",
        "                    )\n",
        "\n",
        "            # Milestone evaluation\n",
        "            if epi in MILESTONES:\n",
        "                vid_dir = os.path.join(tl_base, f\"milestone_{epi}_eps\", f\"seed{seed}\")\n",
        "                os.makedirs(vid_dir, exist_ok=True)\n",
        "                with gpu_timer.track():\n",
        "                    metrics = evaluate(agent, is_ppo, episodes=10)\n",
        "                    _ = evaluate(agent,is_ppo,episodes=1,record=True,record_path=vid_dir,video_prefix=f\"{name}_milestone_{epi}_seed{seed}\")\n",
        "\n",
        "                    # get raw rewards for 10 evaluation episodes\n",
        "                    raw_rewards = evaluate(\n",
        "                        agent,\n",
        "                        is_ppo,\n",
        "                        episodes=10,\n",
        "                        return_raw=True,\n",
        "                        eval_seed=seed\n",
        "                    )\n",
        "                    # compute variance over those 10 episodes\n",
        "                    eval_variance = float(np.var(raw_rewards))\n",
        "                gpu_stats = gpu_timer.summary()\n",
        "\n",
        "                pd.DataFrame([{\n",
        "                \"algorithm\": name,\n",
        "                \"seed\": seed,\n",
        "                \"episode\": epi,\n",
        "                \"mean_reward\": metrics[\"mean_reward\"],\n",
        "                \"success_rate\": metrics[\"success_rate\"],\n",
        "                \"mean_dist\": metrics[\"mean_dist\"],\n",
        "                \"mean_fuel\": metrics[\"mean_fuel\"],\n",
        "                \"sample_efficiency\": sample_eff,\n",
        "                \"training_std\": training_std,\n",
        "                \"eval_variance\": eval_variance,\n",
        "                \"time_sec\": gpu_stats[\"mean_time_sec\"],\n",
        "                \"gpu_bytes\": gpu_stats[\"mean_peak_gpu_bytes\"]\n",
        "                }]).to_csv(master_milestone_log, mode=\"a\", header=False, index=False)\n",
        "\n",
        "\n",
        "    if not is_ppo:\n",
        "        # close the single‐env you created for DQN\n",
        "        env.close()\n",
        "    else:\n",
        "        # close the vectorized env you created for PPO\n",
        "        agent.vec_env.close()\n",
        "\n",
        "    return agent"
      ],
      "metadata": {
        "id": "LO6CRVt6KZee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Trajectory Visualization\n",
        "def plot_trajectory(agent, is_ppo, seed=0):\n",
        "    \"\"\"Run one deterministic episode and plot the descent trajectory (x vs. y).\"\"\"\n",
        "    # Prepare environment\n",
        "    env = make_env(seed=seed)\n",
        "    state, _ = env.reset(seed=seed)  # Explicit reset with seed\n",
        "    positions = []\n",
        "    done = False\n",
        "    for t in range(TRAIN_MAX_STEPS):\n",
        "        positions.append((state[0], state[1]))  # x, y\n",
        "        if is_ppo:\n",
        "            action, _ = agent.predict(state, deterministic=True)\n",
        "        else:\n",
        "            # Ensure deterministic action for DQN\n",
        "            state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                action = int(agent.q_net(state_t).argmax(dim=1).item())\n",
        "        state, r, term, trunc, info = env.step(action)\n",
        "        done = term or trunc\n",
        "        if done:\n",
        "            positions.append((state[0], state[1]))\n",
        "            break\n",
        "    env.close()\n",
        "    # Plot trajectory\n",
        "    xs, ys = zip(*positions)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(xs, ys, marker='o', markersize=3)\n",
        "    plt.title(f\"Descent Trajectory: {'PPO' if is_ppo else 'DQN'} (Seed {seed})\")\n",
        "    plt.xlabel('X position')\n",
        "    plt.ylabel('Y position')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(PLOTS_DIR, f\"trajectory_{'ppo' if is_ppo else 'dqn'}_seed{seed}.png\"))\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "akcxc2T2K0aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Cell 15: Q-Value & Angular Distribution Histograms\n",
        "# def plot_histograms(agent, seed=0, num_samples=1000):\n",
        "#     \"\"\"Sample random states to plot histograms of Q-values and angular velocities.\"\"\"\n",
        "#     # Collect random states from environment\n",
        "#     env = make_env(seed=seed)\n",
        "#     states = []\n",
        "#     for _ in range(num_samples):\n",
        "#         st, _ = env.reset()\n",
        "#         assert len(st) == 8, f\"Expected state_dim=8, got {len(st)}\"\n",
        "#         states.append(st)\n",
        "#     env.close()\n",
        "#     states_arr = np.array(states, dtype=np.float32)\n",
        "\n",
        "#     # Q-value distribution (only for DQN agents)\n",
        "#     if not isinstance(agent, PPOAgentWrapper):\n",
        "#         with torch.no_grad():\n",
        "#             q_vals = agent.q_net(torch.tensor(states_arr))\n",
        "#             q_vals = q_vals.cpu().numpy().flatten()\n",
        "#         plt.figure(figsize=(8, 6))\n",
        "#         plt.hist(q_vals, bins=50, alpha=0.7)\n",
        "#         plt.title('Q-Value Distribution')\n",
        "#         plt.xlabel('Q value')\n",
        "#         plt.ylabel('Frequency')\n",
        "#         plt.grid(True)\n",
        "#         plt.savefig(os.path.join(PLOTS_DIR, 'q_value_distribution.png'))\n",
        "#         plt.close()\n",
        "\n",
        "#     # Angular velocity distribution (state index 4)\n",
        "#     ang_vel = states_arr[:, 4]\n",
        "#     plt.figure(figsize=(8, 6))\n",
        "#     plt.hist(ang_vel, bins=50, alpha=0.7)\n",
        "#     plt.title('Angular Velocity Distribution')\n",
        "#     plt.xlabel('Angular Velocity')\n",
        "#     plt.ylabel('Frequency')\n",
        "#     plt.grid(True)\n",
        "#     plt.savefig(os.path.join(PLOTS_DIR, 'angular_velocity_distribution.png'))\n",
        "#     plt.close()\n",
        "\n",
        "# Cell 15 (updated): Q‐Value & Angular Distribution Histograms\n",
        "def plot_histograms(agent, algorithm: str, seed=0, num_samples=1000):\n",
        "    \"\"\"Sample random states to plot Q‐value & angular‐velocity histograms for one agent.\"\"\"\n",
        "    # 1) Collect states\n",
        "    env = make_env(seed=seed)\n",
        "    states = []\n",
        "    for _ in range(num_samples):\n",
        "        st, _ = env.reset()\n",
        "        states.append(st)\n",
        "    env.close()\n",
        "    states_arr = np.array(states, dtype=np.float32)\n",
        "\n",
        "    # 2) Q‐value distribution (only for DQN)\n",
        "    if not isinstance(agent, PPOAgentWrapper):\n",
        "        with torch.no_grad():\n",
        "            q_v = agent.q_net(torch.tensor(states_arr))\n",
        "            q_v = q_v.cpu().numpy().flatten()\n",
        "        plt.figure(figsize=(8,6))\n",
        "        plt.hist(q_v, bins=50, alpha=0.7)\n",
        "        plt.title(f\"Q‐Value Distribution: {algorithm}\")\n",
        "        plt.xlabel(\"Q value\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        fn = os.path.join(PLOTS_DIR, f\"{algorithm}_q_value_distribution.png\")\n",
        "        plt.savefig(fn)\n",
        "        plt.close()\n",
        "\n",
        "    # 3) Angular velocity distribution (θ̇ ↦ state index 5)\n",
        "    ang_vel = states_arr[:, 5]\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.hist(ang_vel, bins=50, alpha=0.7)\n",
        "    plt.title(f\"Angular Velocity Distribution: {algorithm}\")\n",
        "    plt.xlabel(\"Angular velocity (rad/s)\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    fn = os.path.join(PLOTS_DIR, f\"{algorithm}_angular_velocity_distribution.png\")\n",
        "    plt.savefig(fn)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "VtKO0o7ALE4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: GPU Memory Tracking Utility\n",
        "import contextlib\n",
        "\n",
        "class GPUTimer:\n",
        "    \"\"\"Context manager to track CUDA memory usage and runtime.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.peak_mem = []\n",
        "        self.times = []\n",
        "\n",
        "    @contextlib.contextmanager\n",
        "    def track(self):\n",
        "        \"\"\"Track GPU memory and runtime for a code block.\"\"\"\n",
        "        t0 = time.time()\n",
        "        if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "        yield\n",
        "        t1 = time.time()\n",
        "        self.times.append(t1 - t0)\n",
        "        # Record peak memory if available, else 0\n",
        "        if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
        "            self.peak_mem.append(torch.cuda.max_memory_allocated())\n",
        "        else:\n",
        "            self.peak_mem.append(0)\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"Return mean time and peak GPU memory usage.\"\"\"\n",
        "        return {\n",
        "            'mean_time_sec': np.mean(self.times) if self.times else 0,\n",
        "            'mean_peak_gpu_bytes': np.mean(self.peak_mem) if self.peak_mem else 0\n",
        "        }"
      ],
      "metadata": {
        "id": "UNpO_GvsLHD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 17: Orchestrator\n",
        "import gymnasium\n",
        "\n",
        "gpu_timer = GPUTimer()\n",
        "\n",
        "agents = [\n",
        "    {\"name\": \"Vanilla_DQN\", \"factory\": make_vanilla_dqn_agent, \"is_ppo\": False},\n",
        "    {\"name\": \"Double_DQN\", \"factory\": make_double_dqn_agent, \"is_ppo\": False},\n",
        "    {\"name\": \"Dueling_DQN\", \"factory\": make_dueling_dqn_agent, \"is_ppo\": False},\n",
        "    {\"name\": \"PER_DQN\", \"factory\": make_per_dqn_agent, \"is_ppo\": False},\n",
        "    {\"name\": \"PPO\", \"factory\": make_ppo_agent, \"is_ppo\": True},\n",
        "]\n",
        "\n",
        "# Train each agent with GPU monitoring\n",
        "for agent_info in agents:\n",
        "    name = agent_info[\"name\"]\n",
        "    factory = agent_info[\"factory\"]\n",
        "    is_ppo = agent_info[\"is_ppo\"]\n",
        "    seed = 0\n",
        "    print(f\"Starting training for {name}...\")\n",
        "\n",
        "    try:\n",
        "        if is_ppo:\n",
        "            # let the PPO factory build its own seeded VecEnv\n",
        "            agent = factory(seed=0)\n",
        "        else:\n",
        "            env   = make_env(seed=0)\n",
        "            agent = factory(env)\n",
        "\n",
        "        with gpu_timer.track():\n",
        "          agent = train_and_snapshot(name, factory, is_ppo=is_ppo)\n",
        "\n",
        "        gpu_summary = gpu_timer.summary()\n",
        "        print(f\"{name} GPU Summary: {gpu_summary}\")\n",
        "\n",
        "        # — Post‐training 10‐episode evaluation —\n",
        "        post_train_metrics = evaluate(agent, is_ppo, episodes=10, eval_seed=0)\n",
        "        print(f\"{name} Post Training → mean_reward: {post_train_metrics['mean_reward']}, \"\n",
        "        f\"success_rate: {post_train_metrics['success_rate']:.2f}, \"\n",
        "        f\"mean_dist: {post_train_metrics['mean_dist']}\")\n",
        "\n",
        "        # close DQN eval env\n",
        "        if not is_ppo:\n",
        "            env.close()\n",
        "\n",
        "        plot_trajectory(agent, is_ppo, seed=0)\n",
        "        # plot_histograms(agent, seed=0)\n",
        "        plot_histograms(agent, name, seed=0)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Training failed for {name}: {e}\")\n",
        "        continue\n",
        "print(\"✅ Training completed for all agents.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmBy0Ko7Lg0M",
        "outputId": "71529126-fd77-449b-954c-ad7d93fa04dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training for Vanilla_DQN...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
            "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
            "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
            "/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_10_eps/seed0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/moviepy/config_defaults.py:1: DeprecationWarning: invalid escape sequence '\\P'\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_10_eps/seed0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_50_eps/seed0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_100_eps/seed0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_150_eps/seed0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_250_eps/seed0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_10_eps/seed1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_50_eps/seed1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_100_eps/seed1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_150_eps/seed1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_250_eps/seed1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_10_eps/seed2 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_50_eps/seed2 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_100_eps/seed2 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_150_eps/seed2 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_250_eps/seed2 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vanilla_DQN GPU Summary: {'mean_time_sec': np.float64(2102.551782608032), 'mean_peak_gpu_bytes': np.float64(0.0)}\n",
            "Vanilla_DQN Post Training → mean_reward: 112.84 ± 23.23, success_rate: 0.00, mean_dist: 0.35 ± 0.10\n",
            "Starting training for Double_DQN...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_10_eps/seed0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_50_eps/seed0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_100_eps/seed0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/timeline_comparisons/milestone_150_eps/seed0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Double_DQN GPU Summary: {'mean_time_sec': np.float64(1927.8222323656082), 'mean_peak_gpu_bytes': np.float64(0.0)}\n",
            "Double_DQN Post Training → mean_reward: 87.51 ± 2.93, success_rate: 0.00, mean_dist: 0.69 ± 0.03\n",
            "Starting training for Dueling_DQN...\n",
            "Dueling_DQN GPU Summary: {'mean_time_sec': np.float64(2433.436619202296), 'mean_peak_gpu_bytes': np.float64(0.0)}\n",
            "Dueling_DQN Post Training → mean_reward: 30.99 ± 3.26, success_rate: 0.00, mean_dist: 1.00 ± 0.02\n",
            "Starting training for PER_DQN...\n",
            "PER_DQN GPU Summary: {'mean_time_sec': np.float64(2325.4268735051155), 'mean_peak_gpu_bytes': np.float64(0.0)}\n",
            "PER_DQN Post Training → mean_reward: -521.99 ± 22.11, success_rate: 0.00, mean_dist: 1.01 ± 0.04\n",
            "Starting training for PPO...\n",
            "Moviepy - Building video /content/videos/timeline_comparisons/milestone_10_eps/seed0/PPO_milestone_10_seed0-step-0-to-step-250.mp4.\n",
            "Moviepy - Writing video /content/videos/timeline_comparisons/milestone_10_eps/seed0/PPO_milestone_10_seed0-step-0-to-step-250.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/timeline_comparisons/milestone_10_eps/seed0/PPO_milestone_10_seed0-step-0-to-step-250.mp4\n",
            "Saving video to /content/videos/timeline_comparisons/milestone_50_eps/seed0/PPO_milestone_50_seed0-step-0-to-step-250.mp4\n",
            "Moviepy - Building video /content/videos/timeline_comparisons/milestone_50_eps/seed0/PPO_milestone_50_seed0-step-0-to-step-250.mp4.\n",
            "Moviepy - Writing video /content/videos/timeline_comparisons/milestone_50_eps/seed0/PPO_milestone_50_seed0-step-0-to-step-250.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/timeline_comparisons/milestone_50_eps/seed0/PPO_milestone_50_seed0-step-0-to-step-250.mp4\n",
            "Saving video to /content/videos/timeline_comparisons/milestone_100_eps/seed0/PPO_milestone_100_seed0-step-0-to-step-250.mp4\n",
            "Moviepy - Building video /content/videos/timeline_comparisons/milestone_100_eps/seed0/PPO_milestone_100_seed0-step-0-to-step-250.mp4.\n",
            "Moviepy - Writing video /content/videos/timeline_comparisons/milestone_100_eps/seed0/PPO_milestone_100_seed0-step-0-to-step-250.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/timeline_comparisons/milestone_100_eps/seed0/PPO_milestone_100_seed0-step-0-to-step-250.mp4\n",
            "Saving video to /content/videos/timeline_comparisons/milestone_150_eps/seed0/PPO_milestone_150_seed0-step-0-to-step-250.mp4\n",
            "Moviepy - Building video /content/videos/timeline_comparisons/milestone_150_eps/seed0/PPO_milestone_150_seed0-step-0-to-step-250.mp4.\n",
            "Moviepy - Writing video /content/videos/timeline_comparisons/milestone_150_eps/seed0/PPO_milestone_150_seed0-step-0-to-step-250.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/timeline_comparisons/milestone_150_eps/seed0/PPO_milestone_150_seed0-step-0-to-step-250.mp4\n",
            "Saving video to /content/videos/timeline_comparisons/milestone_250_eps/seed0/PPO_milestone_250_seed0-step-0-to-step-250.mp4\n",
            "Moviepy - Building video /content/videos/timeline_comparisons/milestone_250_eps/seed0/PPO_milestone_250_seed0-step-0-to-step-250.mp4.\n",
            "Moviepy - Writing video /content/videos/timeline_comparisons/milestone_250_eps/seed0/PPO_milestone_250_seed0-step-0-to-step-250.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/timeline_comparisons/milestone_250_eps/seed0/PPO_milestone_250_seed0-step-0-to-step-250.mp4\n",
            "Saving video to /content/videos/timeline_comparisons/milestone_10_eps/seed1/PPO_milestone_10_seed1-step-0-to-step-250.mp4\n",
            "Moviepy - Building video /content/videos/timeline_comparisons/milestone_10_eps/seed1/PPO_milestone_10_seed1-step-0-to-step-250.mp4.\n",
            "Moviepy - Writing video /content/videos/timeline_comparisons/milestone_10_eps/seed1/PPO_milestone_10_seed1-step-0-to-step-250.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/timeline_comparisons/milestone_10_eps/seed1/PPO_milestone_10_seed1-step-0-to-step-250.mp4\n",
            "Saving video to /content/videos/timeline_comparisons/milestone_50_eps/seed1/PPO_milestone_50_seed1-step-0-to-step-250.mp4\n",
            "Moviepy - Building video /content/videos/timeline_comparisons/milestone_50_eps/seed1/PPO_milestone_50_seed1-step-0-to-step-250.mp4.\n",
            "Moviepy - Writing video /content/videos/timeline_comparisons/milestone_50_eps/seed1/PPO_milestone_50_seed1-step-0-to-step-250.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/timeline_comparisons/milestone_50_eps/seed1/PPO_milestone_50_seed1-step-0-to-step-250.mp4\n",
            "Saving video to /content/videos/timeline_comparisons/milestone_100_eps/seed1/PPO_milestone_100_seed1-step-0-to-step-250.mp4\n",
            "Moviepy - Building video /content/videos/timeline_comparisons/milestone_100_eps/seed1/PPO_milestone_100_seed1-step-0-to-step-250.mp4.\n",
            "Moviepy - Writing video /content/videos/timeline_comparisons/milestone_100_eps/seed1/PPO_milestone_100_seed1-step-0-to-step-250.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/timeline_comparisons/milestone_100_eps/seed1/PPO_milestone_100_seed1-step-0-to-step-250.mp4\n",
            "Moviepy - Building video /content/videos/timeline_comparisons/milestone_150_eps/seed1/PPO_milestone_150_seed1-step-0-to-step-250.mp4.\n",
            "Moviepy - Writing video /content/videos/timeline_comparisons/milestone_150_eps/seed1/PPO_milestone_150_seed1-step-0-to-step-250.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/timeline_comparisons/milestone_150_eps/seed1/PPO_milestone_150_seed1-step-0-to-step-250.mp4\n",
            "Saving video to /content/videos/timeline_comparisons/milestone_250_eps/seed1/PPO_milestone_250_seed1-step-0-to-step-250.mp4\n",
            "Moviepy - Building video /content/videos/timeline_comparisons/milestone_250_eps/seed1/PPO_milestone_250_seed1-step-0-to-step-250.mp4.\n",
            "Moviepy - Writing video /content/videos/timeline_comparisons/milestone_250_eps/seed1/PPO_milestone_250_seed1-step-0-to-step-250.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/timeline_comparisons/milestone_250_eps/seed1/PPO_milestone_250_seed1-step-0-to-step-250.mp4\n",
            "Saving video to /content/videos/timeline_comparisons/milestone_10_eps/seed2/PPO_milestone_10_seed2-step-0-to-step-250.mp4\n",
            "Moviepy - Building video /content/videos/timeline_comparisons/milestone_10_eps/seed2/PPO_milestone_10_seed2-step-0-to-step-250.mp4.\n",
            "Moviepy - Writing video /content/videos/timeline_comparisons/milestone_10_eps/seed2/PPO_milestone_10_seed2-step-0-to-step-250.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/timeline_comparisons/milestone_10_eps/seed2/PPO_milestone_10_seed2-step-0-to-step-250.mp4\n",
            "Saving video to /content/videos/timeline_comparisons/milestone_50_eps/seed2/PPO_milestone_50_seed2-step-0-to-step-250.mp4\n",
            "Moviepy - Building video /content/videos/timeline_comparisons/milestone_50_eps/seed2/PPO_milestone_50_seed2-step-0-to-step-250.mp4.\n",
            "Moviepy - Writing video /content/videos/timeline_comparisons/milestone_50_eps/seed2/PPO_milestone_50_seed2-step-0-to-step-250.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/timeline_comparisons/milestone_50_eps/seed2/PPO_milestone_50_seed2-step-0-to-step-250.mp4\n",
            "Saving video to /content/videos/timeline_comparisons/milestone_100_eps/seed2/PPO_milestone_100_seed2-step-0-to-step-250.mp4\n",
            "Moviepy - Building video /content/videos/timeline_comparisons/milestone_100_eps/seed2/PPO_milestone_100_seed2-step-0-to-step-250.mp4.\n",
            "Moviepy - Writing video /content/videos/timeline_comparisons/milestone_100_eps/seed2/PPO_milestone_100_seed2-step-0-to-step-250.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/timeline_comparisons/milestone_100_eps/seed2/PPO_milestone_100_seed2-step-0-to-step-250.mp4\n",
            "Saving video to /content/videos/timeline_comparisons/milestone_150_eps/seed2/PPO_milestone_150_seed2-step-0-to-step-250.mp4\n",
            "Moviepy - Building video /content/videos/timeline_comparisons/milestone_150_eps/seed2/PPO_milestone_150_seed2-step-0-to-step-250.mp4.\n",
            "Moviepy - Writing video /content/videos/timeline_comparisons/milestone_150_eps/seed2/PPO_milestone_150_seed2-step-0-to-step-250.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/timeline_comparisons/milestone_150_eps/seed2/PPO_milestone_150_seed2-step-0-to-step-250.mp4\n",
            "Saving video to /content/videos/timeline_comparisons/milestone_250_eps/seed2/PPO_milestone_250_seed2-step-0-to-step-250.mp4\n",
            "Moviepy - Building video /content/videos/timeline_comparisons/milestone_250_eps/seed2/PPO_milestone_250_seed2-step-0-to-step-250.mp4.\n",
            "Moviepy - Writing video /content/videos/timeline_comparisons/milestone_250_eps/seed2/PPO_milestone_250_seed2-step-0-to-step-250.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/timeline_comparisons/milestone_250_eps/seed2/PPO_milestone_250_seed2-step-0-to-step-250.mp4\n",
            "PPO GPU Summary: {'mean_time_sec': np.float64(2432.4859722137453), 'mean_peak_gpu_bytes': np.float64(0.0)}\n",
            "PPO Post Training → mean_reward: 56.72 ± 0.00, success_rate: 0.00, mean_dist: 0.98 ± 0.00\n",
            "✅ Training completed for all agents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 18: Utilities\n",
        "def compute_sample_efficiency(log_df, reward_threshold=200):\n",
        "    \"\"\"Compute steps to reach a reward threshold for sample efficiency.\"\"\"\n",
        "    first_success = log_df[log_df['reward'] >= reward_threshold]['steps'].cumsum()\n",
        "    return first_success.iloc[0] if not first_success.empty else float('inf')\n",
        "\n",
        "def compute_training_stability(log_df, window=10):\n",
        "    \"\"\"Compute variance of rewards over the last N episodes.\"\"\"\n",
        "    if len(log_df) < window:\n",
        "        return 0.0\n",
        "    return np.var(log_df['reward'].tail(window))\n",
        "\n",
        "def validate_log_file(file_path):\n",
        "    \"\"\"Validate CSV log file integrity.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        required_cols = ['algorithm', 'seed', 'episode', 'reward']\n",
        "        if not all(col in df.columns for col in required_cols):\n",
        "            print(f\"⚠ Missing required columns in {file_path}\")\n",
        "            return None\n",
        "        if df.empty or df['reward'].isna().all():\n",
        "            print(f\"⚠ Empty or invalid data in {file_path}\")\n",
        "            return None\n",
        "        return df\n",
        "    except (pd.errors.ParserError, pd.errors.EmptyDataError):\n",
        "        print(f\"⚠ Corrupted or empty file: {file_path}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "zi7a61XBLik1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 19: Post-Training Evaluation & Plots (v1)\n",
        "import glob\n",
        "\n",
        "all_logs = glob.glob(os.path.join(BASE_LOG_DIR, \"master_full_training.csv\"))\n",
        "\n",
        "if not all_logs:\n",
        "  print(f\"⚠ No full training logs in '{BASE_LOG_DIR}'. Run training cells first.\")\n",
        "else:\n",
        "  df_train = pd.read_csv(all_logs[0])\n",
        "# Convert to numeric\n",
        "  for col in ['episode', 'reward', 'time_sec', 'sample_efficiency', 'training_std']:\n",
        "    df_train[col] = pd.to_numeric(df_train[col], errors='coerce')\n",
        "  df_train = df_train.dropna(subset=['episode', 'reward'])\n",
        "\n",
        "  # Plot reward learning curve\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  df_train.groupby(['episode', 'algorithm'])['reward'].mean().unstack('algorithm').plot(title='Mean Reward vs Episode')\n",
        "  plt.xlabel('Episode')\n",
        "  plt.ylabel('Mean Reward')\n",
        "  plt.savefig(os.path.join(PLOTS_DIR, 'learning_curve_reward.png'))\n",
        "  plt.close()\n",
        "\n",
        "  # Plot fuel vs reward\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.scatter(df_train['reward'], df_train['fuel'])\n",
        "  plt.title('Reward vs Fuel per Episode')\n",
        "  plt.xlabel('Reward')\n",
        "  plt.ylabel('Remaining Fuel')\n",
        "  plt.savefig(os.path.join(PLOTS_DIR, 'reward_vs_fuel.png'))\n",
        "  plt.close()\n",
        "\n",
        "# Load milestone metrics\n",
        "m_files = glob.glob(os.path.join(BASE_LOG_DIR, \"master_milestone_metrics.csv\"))\n",
        "\n",
        "if not m_files:\n",
        "  print(f\"⚠ No milestone metrics found. Run Cell 13 first.\")\n",
        "else:\n",
        "  df_milestones = pd.read_csv(m_files[0])\n",
        "  df_milestones = df_milestones[df_milestones['episode'] != 'episode'].reset_index(drop=True)\n",
        "  raw = df_milestones['mean_reward'].astype(str)\n",
        "  df_milestones['mean_reward'] = raw.str.split(' ± ').str[0].astype(float)\n",
        "  df_milestones['std_reward']    = raw.str.split(' ± ').str[1].astype(float)\n",
        "\n",
        "# Convert to numeric\n",
        "  for col in ['episode', 'mean_reward', 'std_reward', 'success_rate', 'mean_fuel', 'sample_efficiency','training_std', 'eval_variance', 'time_sec', 'gpu_bytes']:\n",
        "    df_milestones[col] = pd.to_numeric(df_milestones[col], errors='coerce')\n",
        "  df_milestones = df_milestones.dropna(subset=['episode', 'mean_reward'])\n",
        "\n",
        "  # Bar: reward by milestone\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  df_milestones.groupby(['episode', 'algorithm'])['mean_reward'].mean().unstack('algorithm').plot(kind='bar', title='Mean Reward by Milestone')\n",
        "  plt.xlabel('Milestone')\n",
        "  plt.ylabel('Mean Reward')\n",
        "  plt.savefig(os.path.join(PLOTS_DIR, 'milestones_reward.png'))\n",
        "  plt.close()\n",
        "\n",
        "  # Bar: success rate by milestone\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  df_milestones.groupby(['episode', 'algorithm'])['success_rate'].mean().unstack('algorithm').plot(kind='bar', title='Success Rate by Milestone')\n",
        "  plt.xlabel('Milestone')\n",
        "  plt.ylabel('Success Rate')\n",
        "  plt.savefig(os.path.join(PLOTS_DIR, 'milestones_success_rate.png'))\n",
        "  plt.close()\n",
        "\n",
        "  # Bar: sample efficiency by milestone\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  df_milestones.groupby(['episode', 'algorithm'])['sample_efficiency'].mean().unstack('algorithm').plot(kind='bar', title='Sample Efficiency by Milestone')\n",
        "  plt.xlabel('Milestone')\n",
        "  plt.ylabel('Steps to Reward ≥ 200')\n",
        "  plt.savefig(os.path.join(PLOTS_DIR, 'milestones_sample_efficiency.png'))\n",
        "  plt.close()\n",
        "\n",
        "  # Bar: Compute cost by time\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.bar(df_milestones['algorithm'], df_milestones['time_sec'])\n",
        "  plt.title('Compute Cost: Wall-Clock Time')\n",
        "  plt.xlabel('Algorithm')\n",
        "  plt.ylabel('Time (s)')\n",
        "  plt.savefig(os.path.join(PLOTS_DIR, 'compute_cost_time.png'))\n",
        "  plt.close()\n",
        "\n",
        "  # Scatter: reward vs time\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.scatter(df_train['reward'], df_train['time_sec'])\n",
        "  plt.title('Reward vs Time per Episode')\n",
        "  plt.xlabel('Reward')\n",
        "  plt.ylabel('Time (s)')\n",
        "  plt.savefig(os.path.join(PLOTS_DIR, 'reward_vs_time.png'))\n",
        "  plt.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "0_zr5XwlKw55",
        "outputId": "2e94fcf9-2082-4416-941a-d60ad2bae63f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Cell 20: Post-Training Evaluation & Plots (v2)\n",
        "# import glob\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # ——————— Load full‐training log ———————\n",
        "# all_logs = glob.glob(os.path.join(BASE_LOG_DIR, \"master_full_training.csv\"))\n",
        "# if not all_logs:\n",
        "#     print(f\"⚠ No full training logs in '{BASE_LOG_DIR}'. Run training first.\")\n",
        "# else:\n",
        "#     df_train = pd.read_csv(all_logs[0])\n",
        "#     # numeric conversions\n",
        "#     for col in ['episode','reward','time_sec','sample_efficiency','training_std','fuel']:\n",
        "#         df_train[col] = pd.to_numeric(df_train[col], errors='coerce')\n",
        "#     df_train.dropna(subset=['episode','reward'], inplace=True)\n",
        "\n",
        "#     # 1) Learning curve (unchanged)\n",
        "#     plt.figure(figsize=(10,6))\n",
        "#     df_train.groupby(['episode','algorithm'])['reward'] \\\n",
        "#             .mean().unstack('algorithm') \\\n",
        "#             .plot(title=\"Mean Reward vs Episode\")\n",
        "#     plt.xlabel(\"Episode\")\n",
        "#     plt.ylabel(\"Mean Reward\")\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(os.path.join(PLOTS_DIR,\"learning_curve_reward.png\"))\n",
        "#     plt.close()\n",
        "\n",
        "#     # 2) Reward vs Fuel, split by algorithm\n",
        "#     plt.figure(figsize=(8,6))\n",
        "#     for algo, grp in df_train.groupby(\"algorithm\"):\n",
        "#         plt.scatter(\n",
        "#             grp[\"reward\"],\n",
        "#             grp[\"fuel\"],\n",
        "#             s=10, alpha=0.6,\n",
        "#             label=algo\n",
        "#         )\n",
        "#     plt.title(\"Reward vs Remaining Fuel per Episode\")\n",
        "#     plt.xlabel(\"Reward\")\n",
        "#     plt.ylabel(\"Remaining Fuel\")\n",
        "#     plt.legend()\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(os.path.join(PLOTS_DIR,\"reward_vs_fuel_by_algo.png\"))\n",
        "#     plt.close()\n",
        "\n",
        "#     # 3) Reward vs Time, split by algorithm\n",
        "#     plt.figure(figsize=(8,6))\n",
        "#     for algo, grp in df_train.groupby(\"algorithm\"):\n",
        "#         plt.scatter(\n",
        "#             grp[\"reward\"],\n",
        "#             grp[\"time_sec\"],\n",
        "#             s=10, alpha=0.6,\n",
        "#             label=algo\n",
        "#         )\n",
        "#     plt.title(\"Reward vs Time per Episode\")\n",
        "#     plt.xlabel(\"Reward\")\n",
        "#     plt.ylabel(\"Time (s)\")\n",
        "#     plt.legend()\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(os.path.join(PLOTS_DIR,\"reward_vs_time_by_algo.png\"))\n",
        "#     plt.close()\n",
        "\n",
        "\n",
        "# # ——————— Load milestone metrics ———————\n",
        "# m_files = glob.glob(os.path.join(BASE_LOG_DIR, \"master_milestone_metrics.csv\"))\n",
        "# if not m_files:\n",
        "#     print(f\"⚠ No milestone metrics found. Run Cell 13 first.\")\n",
        "# else:\n",
        "#     df_m = pd.read_csv(m_files[0])\n",
        "#     # drop possible header‐row artifacts\n",
        "#     df_m = df_m[df_m['episode']!='episode'].reset_index(drop=True)\n",
        "\n",
        "#     # parse out mean ± std\n",
        "#     parts = df_m['mean_reward'].astype(str).str.split(' ± ', expand=True)\n",
        "#     df_m['mean_reward'] = parts[0].astype(float)\n",
        "#     df_m['std_reward']  = parts[1].astype(float)\n",
        "\n",
        "#     # numeric conversions\n",
        "#     for col in ['episode','mean_reward','std_reward','success_rate',\n",
        "#                 'mean_fuel','sample_efficiency','training_std',\n",
        "#                 'eval_variance','time_sec','gpu_bytes']:\n",
        "#         df_m[col] = pd.to_numeric(df_m[col], errors='coerce')\n",
        "#     df_m.dropna(subset=['episode','mean_reward'], inplace=True)\n",
        "\n",
        "#     # 4) Mean reward by milestone with error bars\n",
        "#     plt.figure(figsize=(10,6))\n",
        "#     for algo, grp in df_m.groupby(\"algorithm\"):\n",
        "#         plt.errorbar(\n",
        "#             grp[\"episode\"],\n",
        "#             grp[\"mean_reward\"],\n",
        "#             yerr=grp[\"std_reward\"],\n",
        "#             marker='o', capsize=3,\n",
        "#             label=algo\n",
        "#         )\n",
        "#     plt.title(\"Mean Reward ± Std by Milestone\")\n",
        "#     plt.xlabel(\"Milestone Episode\")\n",
        "#     plt.ylabel(\"Mean Reward\")\n",
        "#     plt.legend()\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(os.path.join(PLOTS_DIR,\"milestones_mean_reward_errorbar.png\"))\n",
        "#     plt.close()\n",
        "\n",
        "#     # 5) Success rate trend\n",
        "#     plt.figure(figsize=(10,6))\n",
        "#     for algo, grp in df_m.groupby(\"algorithm\"):\n",
        "#         plt.plot(\n",
        "#             grp[\"episode\"],\n",
        "#             grp[\"success_rate\"],\n",
        "#             marker='o', linestyle='-',\n",
        "#             label=algo\n",
        "#         )\n",
        "#     plt.title(\"Success Rate by Milestone\")\n",
        "#     plt.xlabel(\"Milestone Episode\")\n",
        "#     plt.ylabel(\"Success Rate\")\n",
        "#     plt.legend()\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(os.path.join(PLOTS_DIR,\"milestones_success_rate_line.png\"))\n",
        "#     plt.close()\n",
        "\n",
        "#     # 6) Sample efficiency trend (log‐scale to handle inf)\n",
        "#     plt.figure(figsize=(10,6))\n",
        "#     for algo, grp in df_m.groupby(\"algorithm\"):\n",
        "#         plt.plot(\n",
        "#             grp[\"episode\"],\n",
        "#             grp[\"sample_efficiency\"],\n",
        "#             marker='o', linestyle='-',\n",
        "#             label=algo\n",
        "#         )\n",
        "#     plt.yscale('log')\n",
        "#     plt.title(\"Sample Efficiency by Milestone (log scale)\")\n",
        "#     plt.xlabel(\"Milestone Episode\")\n",
        "#     plt.ylabel(\"Steps to Reward ≥ 200 (log)\")\n",
        "#     plt.legend()\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(os.path.join(PLOTS_DIR,\"milestones_sample_efficiency_log.png\"))\n",
        "#     plt.close()\n",
        "\n",
        "#     # 7) Compute cost (bar)\n",
        "#     plt.figure(figsize=(8,6))\n",
        "#     cost = df_m.groupby(\"algorithm\")[\"time_sec\"].mean()\n",
        "#     cost.plot(kind='bar')\n",
        "#     plt.title(\"Compute Cost: Wall-Clock Time\")\n",
        "#     plt.xlabel(\"Algorithm\")\n",
        "#     plt.ylabel(\"Time (s)\")\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(os.path.join(PLOTS_DIR,\"compute_cost_time_bar.png\"))\n",
        "#     plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "L26ffg6JL8rw",
        "outputId": "d4e442bb-d9d4-452a-bf50-65a1a6163bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "04hPdHGPx452"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}